{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing Wikipedia (English) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Initilizing phonemizer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6ca5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = \"Configs/config.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b363b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import phonemizer\n",
    "from phonemizer.separator import Separator\n",
    "\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='hi', preserve_punctuation=True,  with_stress=True)\n",
    "separator = Separator(phone='', word='', syllable='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d58c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['dataset_params']['tokenizer']) # you can use any other tokenizers if you want to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2fef4b1-6277-4286-8e08-69760af2bf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['dataset_params']['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5787678-30ea-4405-8d6e-08d7e1ec1443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['मैं', 'स्कूल', 'जा', 'रहा', 'हूँ।'], 'words': ['मैं', 'स्कूल', 'जा', 'रहा', 'हूँ।'], 'phonemes': 'mɛ̃skˈuːlɟˈaːɾˌəhaːhũ'}\n"
     ]
    }
   ],
   "source": [
    "def extract_hindi_features(text):\n",
    "    backend = 'espeak'\n",
    "    language = 'hi'\n",
    "    # Correctly initialize the Separator\n",
    "    separator = Separator(phone='', word='', syllable='')\n",
    "    try:\n",
    "        # Use separate parameters for phone, word, and syllable separators\n",
    "        phonemes = global_phonemizer.phonemize([text], separator=separator, njobs=8)\n",
    "        # Tokenize the text\n",
    "        tokens = []\n",
    "        current_word = \"\"\n",
    "        for char in text:\n",
    "            if char == \" \":\n",
    "                if current_word:\n",
    "                    tokens.append(current_word)\n",
    "                    current_word = \"\"\n",
    "            else:\n",
    "                current_word += char\n",
    "        if current_word:\n",
    "            tokens.append(current_word)\n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'words': text.split(),\n",
    "            'phonemes': phonemes[0]  # phonemize returns a list, so we take the first element\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "text = \"मैं स्कूल जा रहा हूँ।\"\n",
    "result = extract_hindi_features(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1360bd7333e148cebec9aca749f09aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09226baa85743058e863836e68ccb6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/201k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db16dbf9e16540aaa2b04ad834ca4963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33962f7f24b457ebcde7883f6a7e058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c61043c762a4c0698071701f33997e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/230M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e8272605954b0b8ccf8027dc8c185d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/160068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"graelo/wikipedia\", \"20230601.hi\",trust_remote_code=True)['train'] # you can use other version of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca7ca2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = \"./wiki_phoneme\" # set up root directory for multiprocessor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92a578d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "num_shards = 300\n",
    "\n",
    "def process_shard(i):\n",
    "    directory = root_directory + \"/shard_\" + str(i)\n",
    "    if os.path.exists(directory):\n",
    "        print(\"Shard %d already exists!\" % i)\n",
    "        return\n",
    "    print('Processing shard %d ...' % i)\n",
    "    shard = dataset.shard(num_shards=num_shards, index=i)\n",
    "    processed_dataset = shard.map(lambda t: extract_hindi_features(t['text']), remove_columns=['text'])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    processed_dataset.save_to_disk(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f9dcf",
   "metadata": {},
   "source": [
    "#### Note: You will need to run the following cell multiple times to process all shards because some will fail. Depending on how fast you process each shard, you will need to change the timeout to a longer value to make more shards processed before being killed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6a00445-fbbf-46e2-8335-608b1392a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool as ProcessPool\n",
    "from concurrent.futures import TimeoutError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04261364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shard 6 already exists!Shard 2 already exists!Shard 16 already exists!Shard 26 already exists!Shard 22 already exists!Shard 30 already exists!Shard 20 already exists!Shard 10 already exists!Shard 8 already exists!Shard 0 already exists!Shard 14 already exists!Shard 4 already exists!Shard 32 already exists!Shard 40 already exists!Shard 28 already exists!Shard 24 already exists!Shard 18 already exists!Shard 46 already exists!Shard 12 already exists!Shard 34 already exists!Shard 36 already exists!\n",
      "\n",
      "\n",
      "Shard 38 already exists!Shard 50 already exists!Shard 44 already exists!Shard 42 already exists!Shard 48 already exists!\n",
      "Shard 64 already exists!\n",
      "Shard 54 already exists!Shard 52 already exists!Shard 60 already exists!Shard 70 already exists!Shard 62 already exists!Shard 74 already exists!Shard 56 already exists!Shard 58 already exists!\n",
      "Shard 68 already exists!\n",
      "Shard 76 already exists!Shard 66 already exists!\n",
      "\n",
      "\n",
      "Shard 72 already exists!Shard 88 already exists!\n",
      "Shard 78 already exists!\n",
      "\n",
      "Shard 86 already exists!Shard 84 already exists!Shard 92 already exists!\n",
      "\n",
      "Shard 80 already exists!Shard 82 already exists!\n",
      "Shard 94 already exists!Shard 90 already exists!\n",
      "Shard 98 already exists!\n",
      "\n",
      "\n",
      "Shard 17 already exists!Shard 3 already exists!Shard 104 already exists!Shard 114 already exists!Shard 108 already exists!Shard 106 already exists!Shard 100 already exists!\n",
      "\n",
      "\n",
      "Shard 96 already exists!Shard 102 already exists!Shard 116 already exists!Shard 7 already exists!Shard 118 already exists!\n",
      "Shard 110 already exists!Shard 120 already exists!\n",
      "Shard 124 already exists!Shard 112 already exists!Shard 23 already exists!Shard 126 already exists!\n",
      "\n",
      "\n",
      "Shard 27 already exists!\n",
      "\n",
      "Shard 122 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 31 already exists!\n",
      "Shard 19 already exists!\n",
      "\n",
      "Shard 21 already exists!Shard 25 already exists!Shard 5 already exists!\n",
      "Shard 11 already exists!Shard 1 already exists!\n",
      "Shard 9 already exists!\n",
      "Shard 29 already exists!Shard 33 already exists!\n",
      "Shard 15 already exists!\n",
      "\n",
      "Shard 41 already exists!Shard 47 already exists!\n",
      "\n",
      "Shard 13 already exists!Shard 35 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 43 already exists!Shard 51 already exists!\n",
      "Shard 37 already exists!Shard 45 already exists!Shard 39 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 57 already exists!Shard 77 already exists!Shard 99 already exists!Shard 61 already exists!\n",
      "Shard 79 already exists!Shard 81 already exists!\n",
      "Shard 93 already exists!\n",
      "\n",
      "Shard 87 already exists!Shard 71 already exists!\n",
      "Shard 53 already exists!Shard 55 already exists!Shard 85 already exists!Shard 49 already exists!Shard 73 already exists!Shard 67 already exists!Shard 75 already exists!Shard 65 already exists!Shard 69 already exists!Shard 91 already exists!Shard 59 already exists!Shard 89 already exists!Shard 63 already exists!\n",
      "Shard 128 already exists!Shard 130 already exists!\n",
      "Shard 83 already exists!Shard 117 already exists!Shard 115 already exists!\n",
      "Shard 95 already exists!Shard 101 already exists!Shard 119 already exists!Shard 111 already exists!Shard 113 already exists!Shard 105 already exists!Shard 107 already exists!Shard 109 already exists!Shard 134 already exists!\n",
      "Shard 125 already exists!Shard 132 already exists!Shard 97 already exists!Shard 136 already exists!\n",
      "Shard 127 already exists!Shard 103 already exists!Shard 142 already exists!Shard 123 already exists!Shard 140 already exists!Shard 121 already exists!\n",
      "Shard 138 already exists!Shard 156 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 152 already exists!\n",
      "Shard 146 already exists!Shard 154 already exists!\n",
      "Shard 148 already exists!\n",
      "\n",
      "Shard 144 already exists!\n",
      "Shard 158 already exists!Shard 150 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 160 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 164 already exists!\n",
      "Shard 162 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 166 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 170 already exists!\n",
      "\n",
      "\n",
      "Shard 168 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 180 already exists!Shard 174 already exists!Shard 172 already exists!Shard 178 already exists!\n",
      "Shard 176 already exists!Shard 182 already exists!\n",
      "\n",
      "Shard 198 already exists!Shard 184 already exists!Shard 196 already exists!Shard 202 already exists!Shard 192 already exists!Shard 194 already exists!Shard 186 already exists!Shard 131 already exists!Shard 200 already exists!Shard 188 already exists!\n",
      "Shard 190 already exists!Shard 208 already exists!\n",
      "Shard 129 already exists!Shard 214 already exists!Shard 216 already exists!\n",
      "Shard 204 already exists!Shard 206 already exists!Shard 224 already exists!Shard 220 already exists!Shard 212 already exists!Shard 222 already exists!Shard 210 already exists!Shard 135 already exists!\n",
      "Shard 226 already exists!Shard 232 already exists!Shard 230 already exists!\n",
      "Shard 218 already exists!Shard 228 already exists!\n",
      "\n",
      "Shard 236 already exists!Shard 238 already exists!\n",
      "\n",
      "\n",
      "Shard 133 already exists!\n",
      "Shard 143 already exists!Shard 240 already exists!Shard 242 already exists!Shard 244 already exists!Shard 153 already exists!Shard 246 already exists!\n",
      "\n",
      "Shard 252 already exists!\n",
      "\n",
      "Shard 248 already exists!Shard 254 already exists!Shard 250 already exists!Shard 234 already exists!\n",
      "\n",
      "Shard 137 already exists!Shard 157 already exists!\n",
      "\n",
      "Shard 149 already exists!\n",
      "\n",
      "Shard 147 already exists!Shard 151 already exists!Shard 141 already exists!\n",
      "Shard 161 already exists!\n",
      "Shard 155 already exists!Shard 145 already exists!Shard 159 already exists!\n",
      "Shard 163 already exists!Shard 139 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 167 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 165 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 171 already exists!\n",
      "Shard 175 already exists!Shard 181 already exists!Shard 173 already exists!Shard 169 already exists!\n",
      "Shard 183 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 179 already exists!\n",
      "\n",
      "\n",
      "Shard 203 already exists!Shard 199 already exists!Shard 197 already exists!\n",
      "\n",
      "\n",
      "Shard 185 already exists!Shard 195 already exists!Shard 177 already exists!\n",
      "\n",
      "Shard 258 already exists!Shard 201 already exists!Shard 256 already exists!Shard 193 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 187 already exists!\n",
      "Shard 225 already exists!Shard 209 already exists!Shard 217 already exists!\n",
      "\n",
      "\n",
      "Shard 191 already exists!Shard 215 already exists!\n",
      "Shard 189 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 237 already exists!\n",
      "\n",
      "Shard 233 already exists!Shard 264 already exists!Shard 231 already exists!Shard 205 already exists!Shard 266 already exists!Shard 253 already exists!Shard 219 already exists!Shard 270 already exists!Shard 243 already exists!Shard 213 already exists!Shard 260 already exists!Shard 221 already exists!Shard 247 already exists!Shard 239 already exists!Shard 211 already exists!Shard 207 already exists!Shard 249 already exists!Shard 262 already exists!\n",
      "Shard 268 already exists!Shard 272 already exists!Shard 241 already exists!Shard 251 already exists!Shard 229 already exists!Shard 278 already exists!Shard 245 already exists!Shard 255 already exists!Shard 227 already exists!\n",
      "Shard 223 already exists!Shard 235 already exists!\n",
      "Shard 284 already exists!Shard 276 already exists!Shard 286 already exists!Shard 274 already exists!Shard 282 already exists!\n",
      "Shard 292 already exists!Shard 280 already exists!\n",
      "Shard 294 already exists!Shard 296 already exists!\n",
      "Shard 288 already exists!\n",
      "Shard 298 already exists!\n",
      "\n",
      "Shard 290 already exists!\n",
      "\n",
      "Shard 259 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 257 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 271 already exists!\n",
      "Shard 269 already exists!Shard 265 already exists!\n",
      "Shard 263 already exists!Shard 267 already exists!Shard 261 already exists!Shard 273 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shard 287 already exists!Shard 283 already exists!Shard 285 already exists!Shard 279 already exists!Shard 289 already exists!Shard 277 already exists!Shard 275 already exists!Shard 295 already exists!Shard 281 already exists!Shard 293 already exists!Shard 299 already exists!Shard 297 already exists!\n",
      "Shard 291 already exists!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_workers = 64 \n",
    "with ProcessPool(processes=max_workers) as pool:\n",
    "    pool.map(process_shard, range(num_shards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78caee6",
   "metadata": {},
   "source": [
    "### Collect all shards to form the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5899859-3298-43d9-b12e-02ff75da7666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 160068\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d3596-7643-43a7-a81e-99dae1743d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fed1a6b-45c9-4e28-bd2c-d84a934150fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbf789-ba1a-46f7-bbb4-98ac07fffa54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0568da38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard_265 loaded\n",
      "shard_267 loaded\n",
      "shard_285 loaded\n",
      "shard_279 loaded\n",
      "shard_295 loaded\n",
      "shard_284 loaded\n",
      "shard_291 loaded\n",
      "shard_281 loaded\n",
      "shard_266 loaded\n",
      "shard_197 loaded\n",
      "shard_245 loaded\n",
      "shard_229 loaded\n",
      "shard_247 loaded\n",
      "shard_243 loaded\n",
      "shard_159 loaded\n",
      "shard_107 loaded\n",
      "shard_253 loaded\n",
      "shard_299 loaded\n",
      "shard_273 loaded\n",
      "shard_297 loaded\n",
      "shard_191 loaded\n",
      "shard_278 loaded\n",
      "shard_287 loaded\n",
      "shard_289 loaded\n",
      "shard_293 loaded\n",
      "shard_3 loaded\n",
      "shard_283 loaded\n",
      "shard_217 loaded\n",
      "shard_249 loaded\n",
      "shard_294 loaded\n",
      "shard_277 loaded\n",
      "shard_261 loaded\n",
      "shard_271 loaded\n",
      "shard_269 loaded\n",
      "shard_228 loaded\n",
      "shard_275 loaded\n",
      "shard_179 loaded\n",
      "shard_163 loaded\n",
      "shard_290 loaded\n",
      "shard_280 loaded\n",
      "shard_244 loaded\n",
      "shard_196 loaded\n",
      "shard_263 loaded\n",
      "shard_183 loaded\n",
      "shard_225 loaded\n",
      "shard_219 loaded\n",
      "shard_259 loaded\n",
      "shard_137 loaded\n",
      "shard_246 loaded\n",
      "shard_231 loaded\n",
      "shard_257 loaded\n",
      "shard_251 loaded\n",
      "shard_255 loaded\n",
      "shard_235 loaded\n",
      "shard_252 loaded\n",
      "shard_241 loaded\n",
      "shard_69 loaded\n",
      "shard_298 loaded\n",
      "shard_199 loaded\n",
      "shard_296 loaded\n",
      "shard_237 loaded\n",
      "shard_193 loaded\n",
      "shard_272 loaded\n",
      "shard_59 loaded\n",
      "shard_221 loaded\n",
      "shard_2 loaded\n",
      "shard_288 loaded\n",
      "shard_292 loaded\n",
      "shard_286 loaded\n",
      "shard_227 loaded\n",
      "shard_207 loaded\n",
      "shard_215 loaded\n",
      "shard_282 loaded\n",
      "shard_239 loaded\n",
      "shard_248 loaded\n",
      "shard_233 loaded\n",
      "shard_209 loaded\n",
      "shard_268 loaded\n",
      "shard_274 loaded\n",
      "shard_276 loaded\n",
      "shard_270 loaded\n",
      "shard_260 loaded\n",
      "shard_223 loaded\n",
      "shard_178 loaded\n",
      "shard_262 loaded\n",
      "shard_264 loaded\n",
      "shard_171 loaded\n",
      "shard_258 loaded\n",
      "shard_218 loaded\n",
      "shard_175 loaded\n",
      "shard_182 loaded\n",
      "shard_195 loaded\n",
      "shard_185 loaded\n",
      "shard_149 loaded\n",
      "shard_256 loaded\n",
      "shard_254 loaded\n",
      "shard_133 loaded\n",
      "shard_250 loaded\n",
      "shard_37 loaded\n",
      "shard_213 loaded\n",
      "shard_192 loaded\n",
      "shard_211 loaded\n",
      "shard_224 loaded\n",
      "shard_226 loaded\n",
      "shard_136 loaded\n",
      "shard_165 loaded\n",
      "shard_201 loaded\n",
      "shard_206 loaded\n",
      "shard_203 loaded\n",
      "shard_85 loaded\n",
      "shard_242 loaded\n",
      "shard_205 loaded\n",
      "shard_240 loaded\n",
      "shard_93 loaded\n",
      "shard_236 loaded\n",
      "shard_198 loaded\n",
      "shard_161 loaded\n",
      "shard_238 loaded\n",
      "shard_234 loaded\n",
      "shard_232 loaded\n",
      "shard_131 loaded\n",
      "shard_135 loaded\n",
      "shard_153 loaded\n",
      "shard_53 loaded\n",
      "shard_143 loaded\n",
      "shard_220 loaded\n",
      "shard_230 loaded\n",
      "shard_169 loaded\n",
      "shard_157 loaded\n",
      "shard_216 loaded\n",
      "shard_222 loaded\n",
      "shard_45 loaded\n",
      "shard_177 loaded\n",
      "shard_170 loaded\n",
      "shard_189 loaded\n",
      "shard_174 loaded\n",
      "shard_132 loaded\n",
      "shard_181 loaded\n",
      "shard_139 loaded\n",
      "shard_67 loaded\n",
      "shard_214 loaded\n",
      "shard_187 loaded\n",
      "shard_68 loaded\n",
      "shard_210 loaded\n",
      "shard_212 loaded\n",
      "shard_57 loaded\n",
      "shard_194 loaded\n",
      "shard_208 loaded\n",
      "shard_147 loaded\n",
      "shard_129 loaded\n",
      "shard_202 loaded\n",
      "shard_200 loaded\n",
      "shard_21 loaded\n",
      "shard_173 loaded\n",
      "shard_113 loaded\n",
      "shard_51 loaded\n",
      "shard_204 loaded\n",
      "shard_167 loaded\n",
      "shard_41 loaded\n",
      "shard_43 loaded\n",
      "shard_158 loaded\n",
      "shard_142 loaded\n",
      "shard_101 loaded\n",
      "shard_151 loaded\n",
      "shard_190 loaded\n",
      "shard_160 loaded\n",
      "shard_103 loaded\n",
      "shard_97 loaded\n",
      "shard_155 loaded\n",
      "shard_176 loaded\n",
      "shard_188 loaded\n",
      "shard_164 loaded\n",
      "shard_49 loaded\n",
      "shard_152 loaded\n",
      "shard_47 loaded\n",
      "shard_184 loaded\n",
      "shard_141 loaded\n",
      "shard_186 loaded\n",
      "shard_145 loaded\n",
      "shard_180 loaded\n",
      "shard_128 loaded\n",
      "shard_58 loaded\n",
      "shard_65 loaded\n",
      "shard_121 loaded\n",
      "shard_5 loaded\n",
      "shard_89 loaded\n",
      "shard_172 loaded\n",
      "shard_166 loaded\n",
      "shard_31 loaded\n",
      "shard_168 loaded\n",
      "shard_123 loaded\n",
      "shard_7 loaded\n",
      "shard_125 loaded\n",
      "shard_156 loaded\n",
      "shard_162 loaded\n",
      "shard_44 loaded\n",
      "shard_33 loaded\n",
      "shard_96 loaded\n",
      "shard_102 loaded\n",
      "shard_150 loaded\n",
      "shard_27 loaded\n",
      "shard_154 loaded\n",
      "shard_11 loaded\n",
      "shard_105 loaded\n",
      "shard_77 loaded\n",
      "shard_111 loaded\n",
      "shard_148 loaded\n",
      "shard_146 loaded\n",
      "shard_61 loaded\n",
      "shard_134 loaded\n",
      "shard_140 loaded\n",
      "shard_144 loaded\n",
      "shard_48 loaded\n",
      "shard_75 loaded\n",
      "shard_130 loaded\n",
      "shard_106 loaded\n",
      "shard_138 loaded\n",
      "shard_63 loaded\n",
      "shard_42 loaded\n",
      "shard_9 loaded\n",
      "shard_29 loaded\n",
      "shard_36 loaded\n",
      "shard_112 loaded\n",
      "shard_84 loaded\n",
      "shard_40 loaded\n",
      "shard_64 loaded\n",
      "shard_81 loaded\n",
      "shard_109 loaded\n",
      "shard_122 loaded\n",
      "shard_127 loaded\n",
      "shard_52 loaded\n",
      "shard_6 loaded\n",
      "shard_71 loaded\n",
      "shard_92 loaded\n",
      "shard_55 loaded\n",
      "shard_46 loaded\n",
      "shard_91 loaded\n",
      "shard_13 loaded\n",
      "shard_23 loaded\n",
      "shard_117 loaded\n",
      "shard_87 loaded\n",
      "shard_88 loaded\n",
      "shard_39 loaded\n",
      "shard_73 loaded\n",
      "shard_115 loaded\n",
      "shard_10 loaded\n",
      "shard_35 loaded\n",
      "shard_104 loaded\n",
      "shard_15 loaded\n",
      "shard_30 loaded\n",
      "shard_99 loaded\n",
      "shard_17 loaded\n",
      "shard_1 loaded\n",
      "shard_79 loaded\n",
      "shard_83 loaded\n",
      "shard_19 loaded\n",
      "shard_95 loaded\n",
      "shard_119 loaded\n",
      "shard_25 loaded\n",
      "shard_28 loaded\n",
      "shard_32 loaded\n",
      "shard_74 loaded\n",
      "shard_100 loaded\n",
      "shard_80 loaded\n",
      "shard_54 loaded\n",
      "shard_90 loaded\n",
      "shard_60 loaded\n",
      "shard_110 loaded\n",
      "shard_124 loaded\n",
      "shard_22 loaded\n",
      "shard_114 loaded\n",
      "shard_72 loaded\n",
      "shard_38 loaded\n",
      "shard_108 loaded\n",
      "shard_126 loaded\n",
      "shard_98 loaded\n",
      "shard_8 loaded\n",
      "shard_82 loaded\n",
      "shard_116 loaded\n",
      "shard_76 loaded\n",
      "shard_24 loaded\n",
      "shard_4 loaded\n",
      "shard_26 loaded\n",
      "shard_62 loaded\n",
      "shard_12 loaded\n",
      "shard_50 loaded\n",
      "shard_78 loaded\n",
      "shard_120 loaded\n",
      "shard_14 loaded\n",
      "shard_34 loaded\n",
      "shard_16 loaded\n",
      "shard_86 loaded\n",
      "shard_66 loaded\n",
      "shard_18 loaded\n",
      "shard_0 loaded\n",
      "shard_56 loaded\n",
      "shard_118 loaded\n",
      "shard_20 loaded\n",
      "shard_94 loaded\n",
      "shard_70 loaded\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "output = [dI for dI in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory,dI))]\n",
    "datasets = []\n",
    "for o in output:\n",
    "    directory = root_directory + \"/\" + o\n",
    "    try:\n",
    "        shard = load_from_disk(directory)\n",
    "        datasets.append(shard)\n",
    "        print(\"%s loaded\" % o)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1547f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc44d2949b6f4cae8d7ff4f0bf0ef6a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/5 shards):   0%|          | 0/160068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to wikipedia_20220301.hi.processed\n"
     ]
    }
   ],
   "source": [
    "dataset = concatenate_datasets(datasets)\n",
    "dataset.save_to_disk(config['data_folder'])\n",
    "print('Dataset saved to %s' % config['data_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce886d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'tokens', 'words', 'phonemes'],\n",
       "    num_rows: 160068\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dataset size\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e71722da-076a-492d-b797-b2e187cf495a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624649e38be34a93a634d6147a637914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/160068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset and add input_ids\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['phonemes'], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Apply the tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=8)\n",
    "\n",
    "# Add input_ids to the dataset\n",
    "dataset = tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7190772-6625-47c3-88f1-aef2d7458a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'tokens', 'words', 'phonemes', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 160068\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58c68698-9bd0-4f41-86c3-bf1748c6f557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef60851ebbf42f295e6aa7947f0f46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/6 shards):   0%|          | 0/160068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to wikipedia_20220301.hi.processed\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(config['data_folder'])\n",
    "print('Dataset saved to %s' % config['data_folder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6f6f6",
   "metadata": {},
   "source": [
    "### Remove unneccessary tokens from the pre-trained tokenizer\n",
    "The pre-trained tokenizer contains a lot of tokens that are not used in our dataset, so we need to remove these tokens. We also want to predict the word in lower cases because cases do not matter that much for TTS. Pruning the tokenizer is much faster than training a new tokenizer from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da35819a-74c4-4d96-81c1-f340fd8e9de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'url', 'title', 'tokens', 'words', 'phonemes', 'input_ids', 'token_type_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from disk\n",
    "loaded_dataset = load_from_disk(config['data_folder'])\n",
    "\n",
    "# Display the dataset columns\n",
    "print(loaded_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28cec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "\n",
    "file_data = FilePathDataset(dataset)\n",
    "loader = build_dataloader(file_data, num_workers=32, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b7504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = config['dataset_params']['word_separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fcb44a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [00:19<00:00, 65.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# get all unique tokens in the entire dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "unique_index = [special_token]\n",
    "for _, batch in enumerate(tqdm(loader)):\n",
    "    unique_index.extend(batch)\n",
    "    unique_index = list(set(unique_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1445662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1622/1622 [00:00<00:00, 47319.43it/s]\n"
     ]
    }
   ],
   "source": [
    "lower_tokens = []\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])\n",
    "    lower_tokens.append(t)  # Keep the original token\n",
    "\n",
    "lower_tokens = list(set(lower_tokens))  # Ensure uniqueness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dea92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a76cda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1622/1622 [00:00<00:00, 11805.67it/s]\n"
     ]
    }
   ],
   "source": [
    "token_maps = {}\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])  # Decode the token to get the word\n",
    "    new_t = tokenizer.encode(word, add_special_tokens=False)[0]  # Encode the original word without lowercasing\n",
    "    token_maps[t] = {'word': word, 'token': lower_tokens.index(t)}  # Use original token for mapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1c94be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token mapper saved to token_maps.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(config['dataset_params']['token_maps'], 'wb') as handle:\n",
    "    pickle.dump(token_maps, handle)\n",
    "print('Token mapper saved to %s' % config['dataset_params']['token_maps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    }
   ],
   "source": [
    "from dataloader import build_dataloader\n",
    "\n",
    "train_loader = build_dataloader(dataset, batch_size=32, num_workers=0, dataset_config=config['dataset_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70874215",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde929a5-5be5-41f1-9b01-5fe041b99ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
